{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz1GR19nPSBj"
      },
      "source": [
        "# Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mE-o3Pu6UxT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWylZSu1XWOt"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install openai unstructured chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--xZq-rcEU8k"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install openai\n",
        "!pip install unstructured\n",
        "!pip install python-magic\n",
        "!pip install chromadb\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.9/index.html\n",
        "!pip install layoutparser\n",
        "!pip install pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufP42_bOWR1y"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt-get install -y libmagic-dev\n",
        "!apt-get install -y poppler-utils\n",
        "!apt-get install -y tesseract-ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEESr6iEFqi5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDaxtWJcQOwr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install azure-storage-blob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "g3vA8fkZLoZ2",
        "outputId": "cdc87dd1-204a-406b-a004-af16ce80b052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Pillow\n",
            "  Using cached Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "Installing collected packages: Pillow\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.5.0\n",
            "    Uninstalling Pillow-9.5.0:\n",
            "      Successfully uninstalled Pillow-9.5.0\n",
            "Successfully installed Pillow-9.5.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install --force-reinstall Pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlvGW_ggSxnv"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y libreoffice\n",
        "!soffice --version\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18MTKw_VPoLH"
      },
      "source": [
        "# Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1wbvXc_FfGN",
        "outputId": "b8a1d487-296f-4f69-d98d-245f041fdef8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import langchain\n",
        "import openai\n",
        "import os\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain import OpenAI, VectorDBQA\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "# import magic\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJNQ0RiNF7GD"
      },
      "outputs": [],
      "source": [
        "# # Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Define the directory path\n",
        "# directory_path = '/content/drive/MyDrive/FAQ/FAQ'\n",
        "\n",
        "# from langchain.document_loaders import DirectoryLoader, TextLoader, PDFLoader, CSVLoader, PPTLoader\n",
        "\n",
        "# # Initialize the DirectoryLoader\n",
        "# loader = DirectoryLoader(directory_path, show_progress=True)\n",
        "\n",
        "# # Load the documents from the directory\n",
        "# documents = []\n",
        "# for file_path in loader.iterate_files():\n",
        "#     if file_path.endswith('.txt'):\n",
        "#         text_loader = TextLoader(file_path)\n",
        "#         text_document = text_loader.load()\n",
        "#         documents.append(text_document)\n",
        "#     elif file_path.endswith('.pdf'):\n",
        "#         pdf_loader = PDFLoader(file_path)\n",
        "#         pdf_document = pdf_loader.load()\n",
        "#         documents.append(pdf_document)\n",
        "#     elif file_path.endswith('.csv'):\n",
        "#         csv_loader = CSVLoader(file_path)\n",
        "#         csv_document = csv_loader.load()\n",
        "#         documents.append(csv_document)\n",
        "#     elif file_path.endswith('.ppt'):\n",
        "#         ppt_loader = PPTLoader(file_path)\n",
        "#         ppt_document = ppt_loader.load()\n",
        "#         documents.append(ppt_document)\n",
        "\n",
        "# print(f\"You have {len(documents)} document(s) in your data\")\n",
        "\n",
        "# # Further processing of the documents...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2HqnYcaP6Cx"
      },
      "source": [
        "# Loading Documents from Blob Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "bk0_nbE7QUZR",
        "outputId": "163c2cc7-c183-4538-9ea9-c061307835ee"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import AzureBlobStorageContainerLoader\n",
        "loader = AzureBlobStorageContainerLoader(conn_str=\"DefaultEndpointsProtocol=https;AccountName=openaiblob1;EndpointSuffix=core.windows.net \",container=\"openaicontainer1\")\n",
        "document = loader.load()\n",
        "\n",
        "print (f'You have {len(document)} document(s) in your data')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIO-LSsRRF4d"
      },
      "source": [
        "# Text Splitting\n",
        "## ( chunk size= 200 , chunk overlap= 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkyjtnTmUA7c"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=40,\n",
        ")\n",
        "\n",
        "documents = text_splitter.split_documents(docs)\n",
        "documents[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0NEbFwQRdaE"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rwkp_sVU5XB"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "API_KEY= os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh9RMDj-Rkoi"
      },
      "source": [
        "# Vectore Store Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "WW6VjUprVYM9",
        "outputId": "dbc6d18d-64fc-4cce-ced5-595ffef74221"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval_qa/base.py:206: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Lenses are curved pieces of glass that can be used to bend and focus light. Lenses are used in many optical instruments, such as microscopes, telescopes, binoculars, cameras, and other optical devices. They are also used in eyeglasses and contact lenses to help people see better.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import pickle\n",
        "\n",
        "# Create ChromaDB Vector Store from documents and embeddings\n",
        "vectstore = Chroma.from_documents(documents, embeddings)\n",
        "\n",
        "\n",
        "qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=vectstore)\n",
        "\n",
        "query = \"Write abput different lenses in light\"\n",
        "qa.run(query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJGbOAeTYdml"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "vectorstore = Chroma.from_documents(documents, embeddings)\n",
        "query = \"employee data on Adeline Huang?\"\n",
        "docs = vectorstore.similarity_search(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYcPu5psSGOR"
      },
      "source": [
        "# Querying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bt5oPtEMXXq3"
      },
      "outputs": [],
      "source": [
        "qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=vectstore, return_source_documents=True)\n",
        "query = \"Write a summary of football\"\n",
        "result = qa({'query': query})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "ALKD4AFQXcsk",
        "outputId": "55a3aff0-a04d-46b4-8bf2-21f2c6ee6b1d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" The football write-up covers the exhilarating match between two rival teams, showcasing incredible skills, intense competition, and a nail-biting finish. It highlights the star player's outstanding performance, the team's tactical brilliance, and the roaring crowd's electrifying atmosphere. The write-up captures the essence of the game in just 40 words.\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result['result']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiOBOrDsZHpI",
        "outputId": "01a897c3-86a4-4f4c-8c41-f51c922e0332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'query': 'And how is their habitat?', 'result': \" Fuzzle's natural habitat is generally found in woodlands, where there is plenty of food and opportunities for exploration and socializing. These habitats are typically well-protected from the elements and provide plenty of shelter and safety for Fuzzles to thrive.\"}\n"
          ]
        }
      ],
      "source": [
        "# Continue the conversation\n",
        "response = qa({\"query\": \"And how is their habitat?\"})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "eqamIWrJd9lk",
        "outputId": "dfa39ee5-bd16-46f1-c353-98919617ef06"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0af6f24f9081>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAzureBlobStorageContainerLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_splitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecursiveCharacterTextSplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import AzureBlobStorageContainerLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import OpenAI\n",
        "from langchain import RetrievalQA\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Load documents from Azure Blob Storage container\n",
        "loader = AzureBlobStorageContainerLoader(\n",
        "    conn_str=\"DefaultEndpointsProtocol=https;AccountName=translatorstore;EndpointSuffix=core.windows.net\",\n",
        "    container=\"translator-container1\"\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"You have {len(docs)} document(s) in your data\")\n",
        "\n",
        "# Split the documents into smaller chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "documents = text_splitter.split_documents(docs)\n",
        "\n",
        "# Initialize OpenAI Embeddings\n",
        "API_KEY = \"\"\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=API_KEY, model='text-embedding-ada-002')\n",
        "\n",
        "# Create ChromaDB Vector Store from documents and embeddings\n",
        "vectorstore = Chroma.from_documents(documents, embeddings)\n",
        "\n",
        "# # Store the vector store to a file\n",
        "# vectorstore_file = \"vectorstore.pkl\"\n",
        "# with open(vectorstore_file, \"wb\") as f:\n",
        "#     pickle.dump(vectorstore, f)\n",
        "# print(f\"Vector store is stored in {vectorstore_file}\")\n",
        "\n",
        "# Initialize the LLM and QA model\n",
        "llm = OpenAI(openai_api_key=API_KEY, model_name=\"text-davinci-003\",\n",
        "              temperature=0,\n",
        "              max_tokens=300, # Maximal number of tokens returned by the LLM\n",
        "              chain_type=\"stuff\" )\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=OpenAIChat(model='gpt-3.5-turbo'), chain_type='stuff', retriever=vector_store.as_retriever())\n",
        "\n",
        "\n",
        "# # Initialize the MemoryAgent\n",
        "# memory_agent = MemoryAgent()\n",
        "\n",
        "# Previous conversation memory\n",
        "previous_conversation = []\n",
        "\n",
        "# User input and conversation loop\n",
        "print(\"Assistant: Hello! How can I assist you today?\")\n",
        "while True:\n",
        "    query = input(\"User: \")\n",
        "\n",
        "    # Add user query to the conversation\n",
        "    previous_conversation.append({\"role\": \"user\", \"message\": query})\n",
        "\n",
        "    # Retrieve response from the QA model\n",
        "    response = qa.run(query)\n",
        "\n",
        "    # Add QA response to the conversation\n",
        "    previous_conversation.append({\"role\": \"assistant\", \"message\": response})\n",
        "\n",
        "    # Retrieve previous user queries and responses from the conversation\n",
        "    previous_queries = [item[\"message\"] for item in previous_conversation if item[\"role\"] == \"user\"]\n",
        "    previous_responses = [item[\"message\"] for item in previous_conversation if item[\"role\"] == \"assistant\"]\n",
        "\n",
        "    # Print previous conversation\n",
        "    print(\"Previous Conversation:\")\n",
        "    for user_query, assistant_response in zip(previous_queries, previous_responses):\n",
        "        print(\"User:\", user_query)\n",
        "        print(\"Assistant:\", assistant_response)\n",
        "        print()\n",
        "\n",
        "    print(\"Assistant:\", response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsU7gIkxLrSl"
      },
      "outputs": [],
      "source": [
        "\n",
        "   #load the faiss vector store we saved into memory\n",
        "    vectorStore = FAISS.load_local(\"./dbs/documentation/faiss_index\", embeddings)\n",
        "\n",
        "    #use the faiss vector store we saved to search the local document\n",
        "    retriever = vectorStore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
        "\n",
        "    #use the vector store as a retriever\n",
        "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=False)\n",
        "    while True:\n",
        "        query = input('you: ')\n",
        "        if query == 'q':\n",
        "            break\n",
        "        ask_question(qa, query)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
